{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:29.352561800Z",
     "start_time": "2023-11-20T14:48:27.802592700Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.nn import GCNConv\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.argv = ['try_using GCN,ipynb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "            Time         V1         V2        V3        V4        V5  \\\n0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n...          ...        ...        ...       ...       ...       ...   \n284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n\n              V6        V7        V8        V9  ...       V21       V22  \\\n0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n...          ...       ...       ...       ...  ...       ...       ...   \n284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n\n             V23       V24       V25       V26       V27       V28  Amount  \\\n0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n...          ...       ...       ...       ...       ...       ...     ...   \n284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n\n        Class  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n...       ...  \n284802      0  \n284803      0  \n284804      0  \n284805      0  \n284806      0  \n\n[284807 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>284802</th>\n      <td>172786.0</td>\n      <td>-11.881118</td>\n      <td>10.071785</td>\n      <td>-9.834783</td>\n      <td>-2.066656</td>\n      <td>-5.364473</td>\n      <td>-2.606837</td>\n      <td>-4.918215</td>\n      <td>7.305334</td>\n      <td>1.914428</td>\n      <td>...</td>\n      <td>0.213454</td>\n      <td>0.111864</td>\n      <td>1.014480</td>\n      <td>-0.509348</td>\n      <td>1.436807</td>\n      <td>0.250034</td>\n      <td>0.943651</td>\n      <td>0.823731</td>\n      <td>0.77</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284803</th>\n      <td>172787.0</td>\n      <td>-0.732789</td>\n      <td>-0.055080</td>\n      <td>2.035030</td>\n      <td>-0.738589</td>\n      <td>0.868229</td>\n      <td>1.058415</td>\n      <td>0.024330</td>\n      <td>0.294869</td>\n      <td>0.584800</td>\n      <td>...</td>\n      <td>0.214205</td>\n      <td>0.924384</td>\n      <td>0.012463</td>\n      <td>-1.016226</td>\n      <td>-0.606624</td>\n      <td>-0.395255</td>\n      <td>0.068472</td>\n      <td>-0.053527</td>\n      <td>24.79</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284804</th>\n      <td>172788.0</td>\n      <td>1.919565</td>\n      <td>-0.301254</td>\n      <td>-3.249640</td>\n      <td>-0.557828</td>\n      <td>2.630515</td>\n      <td>3.031260</td>\n      <td>-0.296827</td>\n      <td>0.708417</td>\n      <td>0.432454</td>\n      <td>...</td>\n      <td>0.232045</td>\n      <td>0.578229</td>\n      <td>-0.037501</td>\n      <td>0.640134</td>\n      <td>0.265745</td>\n      <td>-0.087371</td>\n      <td>0.004455</td>\n      <td>-0.026561</td>\n      <td>67.88</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284805</th>\n      <td>172788.0</td>\n      <td>-0.240440</td>\n      <td>0.530483</td>\n      <td>0.702510</td>\n      <td>0.689799</td>\n      <td>-0.377961</td>\n      <td>0.623708</td>\n      <td>-0.686180</td>\n      <td>0.679145</td>\n      <td>0.392087</td>\n      <td>...</td>\n      <td>0.265245</td>\n      <td>0.800049</td>\n      <td>-0.163298</td>\n      <td>0.123205</td>\n      <td>-0.569159</td>\n      <td>0.546668</td>\n      <td>0.108821</td>\n      <td>0.104533</td>\n      <td>10.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284806</th>\n      <td>172792.0</td>\n      <td>-0.533413</td>\n      <td>-0.189733</td>\n      <td>0.703337</td>\n      <td>-0.506271</td>\n      <td>-0.012546</td>\n      <td>-0.649617</td>\n      <td>1.577006</td>\n      <td>-0.414650</td>\n      <td>0.486180</td>\n      <td>...</td>\n      <td>0.261057</td>\n      <td>0.643078</td>\n      <td>0.376777</td>\n      <td>0.008797</td>\n      <td>-0.473649</td>\n      <td>-0.818267</td>\n      <td>-0.002415</td>\n      <td>0.013649</td>\n      <td>217.00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>284807 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit_card = pd.read_csv('./creditcard.csv')\n",
    "df_credit_card"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.317909500Z",
     "start_time": "2023-11-20T14:48:29.352561800Z"
    }
   },
   "id": "5fed9dd1ab27a5f7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "x = torch.tensor(df_credit_card.iloc[:, :30].values, dtype=torch.float)\n",
    "y= torch.tensor(df_credit_card['Class'].values, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.349766300Z",
     "start_time": "2023-11-20T14:48:30.317909500Z"
    }
   },
   "id": "d6ab3b8594073757"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n          1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,\n          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n          3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n          4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n          5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,\n          6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n          6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n          7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n          8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10,\n         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11,\n         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12,\n         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13,\n         13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n         14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n         15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17,\n         17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n         18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20,\n         20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22,\n         23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26,\n         27, 27, 28],\n        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  2,  3,  4,  5,  6,  7,  8,\n          9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n         27, 28, 29,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  4,  5,  6,  7,  8,  9,\n         10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n         28, 29,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n         21, 22, 23, 24, 25, 26, 27, 28, 29,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n         15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  7,  8,  9,\n         10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n         28, 29,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n         24, 25, 26, 27, 28, 29,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n         21, 22, 23, 24, 25, 26, 27, 28, 29, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 11, 12, 13, 14, 15, 16, 17,\n         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 12, 13, 14, 15, 16, 17,\n         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 13, 14, 15, 16, 17, 18,\n         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 14, 15, 16, 17, 18, 19, 20,\n         21, 22, 23, 24, 25, 26, 27, 28, 29, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n         24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n         28, 29, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 18, 19, 20,\n         21, 22, 23, 24, 25, 26, 27, 28, 29, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n         28, 29, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 21, 22, 23, 24, 25, 26,\n         27, 28, 29, 22, 23, 24, 25, 26, 27, 28, 29, 23, 24, 25, 26, 27, 28, 29,\n         24, 25, 26, 27, 28, 29, 25, 26, 27, 28, 29, 26, 27, 28, 29, 27, 28, 29,\n         28, 29, 29]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = 30\n",
    "edges = []\n",
    "for i in range(num_nodes):\n",
    "    for j in range(i + 1, num_nodes):\n",
    "        edges.append([i, j])\n",
    "\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "edge_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.428685500Z",
     "start_time": "2023-11-20T14:48:30.349766300Z"
    }
   },
   "id": "71d23a7acdcf9c21"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train nodes: 199364.0\n",
      "Validation nodes: 56961.0\n",
      "Test nodes: 28480.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_samples = 284807\n",
    "\n",
    "# 划分比例\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# 划分数量\n",
    "num_train = int(train_ratio * total_samples)\n",
    "num_val = int(val_ratio * total_samples)\n",
    "num_test = int(test_ratio * total_samples)\n",
    "\n",
    "# 创建掩码\n",
    "train_mask = np.zeros(total_samples)\n",
    "val_mask = np.zeros(total_samples)\n",
    "test_mask = np.zeros(total_samples)\n",
    "\n",
    "# 将相应数量的节点置为1\n",
    "train_mask[:num_train] = 1\n",
    "val_mask[num_train:num_train + num_val] = 1\n",
    "test_mask[num_train + num_val:num_train + num_val + num_test] = 1\n",
    "\n",
    "# 打印掩码中 1 的数量，确保划分正确\n",
    "print(\"Train nodes:\", np.sum(train_mask))\n",
    "print(\"Validation nodes:\", np.sum(val_mask))\n",
    "print(\"Test nodes:\", np.sum(test_mask))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.475564300Z",
     "start_time": "2023-11-20T14:48:30.365507400Z"
    }
   },
   "id": "c2e15ab791a91b2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[284807, 30], edge_index=[2, 435], y=[284807], train_mask=[284807], val_mask=[284807], test_mask=[284807])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.475564300Z",
     "start_time": "2023-11-20T14:48:30.382651200Z"
    }
   },
   "id": "64340439e10730eb"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='Cora')\n",
    "parser.add_argument('--hidden_channels', type=int, default=16)\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--epochs', type=int, default=200)\n",
    "parser.add_argument('--use_gdc',  action='store_true', help='Use GDC')\n",
    "parser.add_argument('--wandb', action='store_true', help='Track experiment')\n",
    "args = parser.parse_args()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.475564300Z",
     "start_time": "2023-11-20T14:48:30.397320300Z"
    }
   },
   "id": "ce201c93d09e2f3e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.475564300Z",
     "start_time": "2023-11-20T14:48:30.413069100Z"
    }
   },
   "id": "f98ea60a1805ec96"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "init_wandb(\n",
    "    name='credit_fraud',\n",
    "    lr = args.lr,\n",
    "    epochs=args.epochs,\n",
    "    hidden_channels=args.hidden_channels,\n",
    "    device=device,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.475564300Z",
     "start_time": "2023-11-20T14:48:30.444245Z"
    }
   },
   "id": "423c08ccdaf3e27"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "if args.use_gdc:\n",
    "    transform = T.GDC(\n",
    "        self_loop_weight=1,\n",
    "        normalization_in='sym',\n",
    "        normalization_out='col',\n",
    "        diffusion_kwargs=dict(method='ppr', alpha=0.05),\n",
    "        sparsification_kwargs=dict(method='topk', k=128, dim=0),\n",
    "        exact=True,\n",
    "    )\n",
    "    data = transform(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.475564300Z",
     "start_time": "2023-11-20T14:48:30.459955400Z"
    }
   },
   "id": "bfcd2eb74848252"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:30.522435300Z",
     "start_time": "2023-11-20T14:48:30.475564300Z"
    }
   },
   "id": "e38030e6daabab06"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.7296, Train: 0.0000, Val: 0.0000, Test: 000\n",
      "Epoch: 002, Loss: 0.6423, Train: 0.0000, Val: 0.0000, Test: 000\n",
      "Epoch: 003, Loss: 0.8304, Train: 0.0000, Val: 0.0000, Test: 000\n",
      "Epoch: 004, Loss: 0.4714, Train: 0.0000, Val: 0.0000, Test: 000\n",
      "Epoch: 005, Loss: 31.2860, Train: 0.0000, Val: 0.0000, Test: 000\n",
      "Epoch: 006, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 007, Loss: 13.5889, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 008, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 009, Loss: 5.3508, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 010, Loss: 0.1758, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 011, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 012, Loss: 0.0036, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 013, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 014, Loss: 0.2830, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 015, Loss: 0.4394, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 016, Loss: 0.0771, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 017, Loss: 0.0655, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 018, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 019, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 020, Loss: 0.0296, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 021, Loss: 0.1630, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 022, Loss: 0.0041, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 023, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 024, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 025, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 026, Loss: 0.0004, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 027, Loss: 0.0736, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 028, Loss: 0.0084, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 029, Loss: 0.0190, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 030, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 031, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 032, Loss: 0.0162, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 033, Loss: 0.0073, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 034, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 035, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 036, Loss: 0.0938, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 037, Loss: 0.1503, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 038, Loss: 0.1834, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 039, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 040, Loss: 0.0024, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 041, Loss: 0.0103, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 042, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 043, Loss: 0.0527, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 044, Loss: 0.0212, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 045, Loss: 0.0018, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 046, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 047, Loss: 0.0161, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 048, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 049, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 050, Loss: 0.0447, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 051, Loss: 0.0064, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 052, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 053, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 054, Loss: 0.0225, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 055, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 056, Loss: 0.0132, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 057, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 058, Loss: 0.0248, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 059, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 060, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 061, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 062, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 063, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 064, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 065, Loss: 0.0001, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 066, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 067, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 068, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 069, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 070, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 071, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 072, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 073, Loss: 0.0002, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 074, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 075, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 076, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 077, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 078, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 079, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 080, Loss: 0.0003, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 081, Loss: 0.0108, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 082, Loss: 0.0003, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 083, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 084, Loss: 0.0001, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 085, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 086, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 087, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 088, Loss: 0.0001, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 089, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 090, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 091, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 092, Loss: 0.0066, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 093, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 094, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 095, Loss: 0.1311, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 096, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 097, Loss: 0.0001, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 098, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 099, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 100, Loss: 0.0008, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 101, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 102, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 103, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 104, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 105, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 106, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 107, Loss: 0.0100, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 108, Loss: 0.0003, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 109, Loss: 0.0014, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 110, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 111, Loss: 0.0005, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 112, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 113, Loss: 0.0033, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 114, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 115, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 116, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 117, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 118, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 119, Loss: 0.0002, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 120, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 121, Loss: 0.0003, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 122, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 123, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 124, Loss: 0.0009, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 125, Loss: 0.0132, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 126, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 127, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 128, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 129, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 130, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 131, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 132, Loss: 0.0058, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 133, Loss: 0.0216, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 134, Loss: 0.0002, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 135, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 136, Loss: 0.0006, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 137, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 138, Loss: 0.0009, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 139, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 140, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 141, Loss: 0.0003, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 142, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 143, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 144, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 145, Loss: 0.0825, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 146, Loss: 0.0003, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 147, Loss: 0.0002, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 148, Loss: 0.0089, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 149, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 150, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 151, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 152, Loss: 0.0019, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 153, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 154, Loss: 0.0001, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 155, Loss: 0.0001, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 156, Loss: 0.0009, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 157, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 158, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 159, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 160, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 161, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 162, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 163, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 164, Loss: 0.0007, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 165, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 166, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 167, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 168, Loss: 0.0120, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 169, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 170, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 171, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 172, Loss: 0.0030, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 173, Loss: 0.0010, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 174, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 175, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 176, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 177, Loss: 0.0050, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 178, Loss: 0.0024, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 179, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 180, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 181, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 182, Loss: 0.1462, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 183, Loss: 0.0001, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 184, Loss: 0.0374, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 185, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 186, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 187, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 188, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 189, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 190, Loss: 0.0001, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 191, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 192, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 193, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 194, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 195, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 196, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 197, Loss: 0.0003, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 198, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 199, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Epoch: 200, Loss: 0.0000, Train: 1.4286, Val: 5.0000, Test: 10.0002\n",
      "Median time per epoch: 0.1884s\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels,\n",
    "                             normalize=not args.use_gdc)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels,\n",
    "                             normalize=not args.use_gdc)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(\n",
    "    in_channels=30,\n",
    "    hidden_channels=args.hidden_channels,\n",
    "    out_channels=1,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.conv1.parameters(), weight_decay=5e-4),\n",
    "    dict(params=model.conv2.parameters(), weight_decay=0)\n",
    "], lr=args.lr)  # Only perform weight-decay on first convolution.\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index, data.edge_attr).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "times = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    times.append(time.time() - start)\n",
    "print(f'Median time per epoch: {torch.tensor(times).median():.4f}s')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:50:35.215217700Z",
     "start_time": "2023-11-20T14:49:57.393039500Z"
    }
   },
   "id": "819ec35aa0c7bb10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:48:31.883277200Z",
     "start_time": "2023-11-20T14:48:31.883277200Z"
    }
   },
   "id": "cd676e9a6101d9e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
